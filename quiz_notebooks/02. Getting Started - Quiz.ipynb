{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1653908d",
   "metadata": {},
   "source": [
    "# 2. Getting Started - Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1109c3fe",
   "metadata": {},
   "source": [
    "## 2.0. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f93cb",
   "metadata": {},
   "source": [
    "Import `polars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db2bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3475d",
   "metadata": {},
   "source": [
    "## 2.1 Question 1: Creating DataFrame from Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8396d2",
   "metadata": {},
   "source": [
    "Given the data dictionary about schoolchildren, create a `pl.DataFrame` and display it. What are the datatypes of each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de06f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>first_name</th><th>last_name</th><th>age</th><th>favorite_subject</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;danny&quot;</td><td>&quot;lang&quot;</td><td>4.5</td><td>&quot;math&quot;</td></tr><tr><td>&quot;stanny&quot;</td><td>&quot;slang&quot;</td><td>4.0</td><td>&quot;english&quot;</td></tr><tr><td>&quot;ranny&quot;</td><td>&quot;rang&quot;</td><td>5.0</td><td>&quot;art&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 4)\n",
       "┌────────────┬───────────┬─────┬──────────────────┐\n",
       "│ first_name ┆ last_name ┆ age ┆ favorite_subject │\n",
       "│ ---        ┆ ---       ┆ --- ┆ ---              │\n",
       "│ str        ┆ str       ┆ f64 ┆ str              │\n",
       "╞════════════╪═══════════╪═════╪══════════════════╡\n",
       "│ danny      ┆ lang      ┆ 4.5 ┆ math             │\n",
       "│ stanny     ┆ slang     ┆ 4.0 ┆ english          │\n",
       "│ ranny      ┆ rang      ┆ 5.0 ┆ art              │\n",
       "└────────────┴───────────┴─────┴──────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dictionary = {\n",
    "    \"first_name\": [\"danny\", \"stanny\", \"ranny\"],\n",
    "    \"last_name\": [\"lang\", \"slang\", \"rang\"],\n",
    "    \"age\": [4.5, 4., 5.],\n",
    "    \"favorite_subject\": [\"math\", \"english\", \"art\"],\n",
    "}\n",
    "#### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a4fe9-d2c5-4094-8427-fe226b59257b",
   "metadata": {},
   "source": [
    "1. (`str`, `str`, `f64`, `str`)\n",
    "2. (`str`, `str`, `f64`, `cat`)\n",
    "3. (`str`, `str`, `i64`, `cat`)\n",
    "4. (`str`, `i64`, `cat`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21327f-88f1-469c-845d-fb19772ffe24",
   "metadata": {},
   "source": [
    "## 2.2 Question 2: Loading CSV with Schema Override"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1e9fb-c6a2-491c-a371-4ab700ae9073",
   "metadata": {},
   "source": [
    "In the module, we loaded data from the CSV file, overriding the schema of the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime` and loading them as a `pl.Datetime` datatype. Now, override the schema to load them as a `pl.Date` datatype. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05bd3381-05d6-43d2-90da-0a3fec4f1394",
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "could not parse `2024-03-27T05:51:45.000000000` as dtype `date` at column 'tpep_pickup_datetime' (column number 2)\n\nThe current offset in the file is 323447780 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `dtypes` argument\n- setting `ignore_errors` to `True`,\n- adding `2024-03-27T05:51:45.000000000` to the `null_values` list.\n\nOriginal error: ```could not convert pattern```",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/yellow_tripdata_2024-03.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#### YOUR CODE HERE\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtpep_pickup_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtpep_dropoff_datetime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv(\n",
    "    \"../data/yellow_tripdata_2024-03.csv\",\n",
    "    schema_overrides={\n",
    "        #### YOUR CODE HERE\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4c51b-bf9e-464d-a1ba-df033288119a",
   "metadata": {},
   "source": [
    "1. All the data is loaded, and the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime` are loaded as a `str` datatype\n",
    "2. All the data is loaded, and the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime` are loaded as a `datetime` datatype\n",
    "3. The data doesn't load\n",
    "4. All the data is loaded, and the columns `tpep_pickup_datetime` and `tpep_dropoff_datetime` are loaded as a `date` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad10071-7303-4337-b7d1-f875108c1259",
   "metadata": {},
   "source": [
    "## 2.3 Question 3: LazyFrame vs DataFrame Selection Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35adfe5-d9c6-45b6-94a0-338dae32ee04",
   "metadata": {},
   "source": [
    "In the module, you saw that selecting columns from a `LazyFrame` is about two or three times faster than selecting columns from a `DataFrame` when data is loaded from a CSV file. However, we only did this for CSV, not for Parquet. Which file type do you think would see a greater speedup when selecting on a `DataFrame` than selecting on a `LazyFrame`: CSV or Parquet and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfbeda-7b9c-407d-89e5-f828dcbee773",
   "metadata": {},
   "source": [
    "1. CSV, because it's a simpler file type\n",
    "2. CSV, because it's an older file format and thus the Polars code for interacting with it is better developed\n",
    "3. Parquet, because both Polars and Parquet are built on the Apache Arrow memory model and thus the Polars development team has spent more time developing the functionality associated with Parquet, making its IO operations faster\n",
    "4. Parquet, because Parquet files keep data from the same column in the same location in memory and thus, when the select gets pushed down to the read operation of LazyFrame, the input engine can skip the unnecessary columns' data faster than it can for a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f21b9-2d8e-475c-bfdb-cc43deaf7e82",
   "metadata": {},
   "source": [
    "## 2.4 Question 4: Highest Null Count in Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1deb41c-4e13-4e31-b0a1-b8030f9e208f",
   "metadata": {},
   "source": [
    "Inspect the dataset with `df.describe()`. What is the highest `null_count` of any column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51aef678-0600-4811-bd57-d262a5b9bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac632fa-0527-4b37-af52-eb93ef0dca22",
   "metadata": {},
   "source": [
    "1. 3582628\n",
    "2. 426190\n",
    "3. 0\n",
    "4. 176836"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
