{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1653908d",
   "metadata": {},
   "source": [
    "# 10. Summative - Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a37bab",
   "metadata": {},
   "source": [
    "## 10.0. Import `polars` and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2da0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pl.Config.set_tbl_rows(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f2cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_column_rename_mapping = {\n",
    "    \"LocationID\": \"location_id\",\n",
    "    \"Borough\": \"borough\",\n",
    "    \"Zone\": \"zone\",\n",
    "}\n",
    "zones_df = (\n",
    "    pl.read_parquet(\"../data/taxi_zone_lookup.parquet\")\n",
    "    .rename(zone_column_rename_mapping)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb7a494-c830-47a0-94a5-fc4e303a92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_rides_column_rename_mapping = {\n",
    "    \"VendorID\": \"vendor_id\",\n",
    "    \"RatecodeID\": \"ratecode_id\",\n",
    "    \"PULocationID\": \"pu_location_id\",\n",
    "    \"DOLocationID\": \"do_location_id\",\n",
    "    \"Airport_fee\": \"airport_fee\",\n",
    "}\n",
    "\n",
    "rides_df_raw = (\n",
    "    pl.read_parquet(\"../data/yellow_tripdata_2024-03.parquet\")\n",
    "    .rename(yellow_rides_column_rename_mapping)\n",
    "    .join(\n",
    "        zones_df.select(pl.all().name.prefix(\"pu_\")),\n",
    "        on=\"pu_location_id\",\n",
    "    )\n",
    "    .join(\n",
    "        zones_df.select(pl.all().name.prefix(\"do_\")),\n",
    "        on=\"do_location_id\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0bf1d-bd6b-4d89-a630-a488ab427ae3",
   "metadata": {},
   "source": [
    "## 10.1 Question 1: Identifying Data Types in a New DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be274606-1dbd-4ce0-897d-8df5ffba5254",
   "metadata": {},
   "source": [
    "Create a DataFrame from the data. Which of the following datatypes can be found in the resultant DataFrame? Select all that apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63eb221-b16b-4d1e-9975-656b7fab92ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>street</th><th>street_number</th></tr><tr><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>&quot;Thoreau Rd&quot;</td><td>17</td></tr><tr><td>2</td><td>&quot;Flanders St&quot;</td><td>18</td></tr><tr><td>3</td><td>&quot;Candy Ave&quot;</td><td>39</td></tr><tr><td>4</td><td>&quot;State St&quot;</td><td>30</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 3)\n",
       "┌─────┬─────────────┬───────────────┐\n",
       "│ id  ┆ street      ┆ street_number │\n",
       "│ --- ┆ ---         ┆ ---           │\n",
       "│ i64 ┆ str         ┆ i64           │\n",
       "╞═════╪═════════════╪═══════════════╡\n",
       "│ 1   ┆ Thoreau Rd  ┆ 17            │\n",
       "│ 2   ┆ Flanders St ┆ 18            │\n",
       "│ 3   ┆ Candy Ave   ┆ 39            │\n",
       "│ 4   ┆ State St    ┆ 30            │\n",
       "└─────┴─────────────┴───────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_data_dict = {\n",
    "    \"id\": [1, 2, 3, 4, ],\n",
    "    \"street\": [\"Thoreau Rd\", \"Flanders St\", \"Candy Ave\", \"State St\", ],\n",
    "    \"street_number\": [17, 18, 39, 30, ]\n",
    "}\n",
    "#### YOUR CODE HERE\n",
    "pl.DataFrame(address_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e095d-44e6-475c-ac48-2ec7fb381a9a",
   "metadata": {},
   "source": [
    "1. `i64` - correct - Both `id` and `street_number` get loaded as `i64`. See Module 2, \"Getting Started.\"\n",
    "2. `u64` - incorrect - Though the values of both `street_number` and `id` appear to be strictly positive, Polars's default behavior is to load integers as `i64`. See Module 2, \"Getting Started.\"\n",
    "3. `i8` - incorrect - Though the values of both `street_number` and `id` appear to fit in the range of 8-bit values, Polars's default behavior is to load integers as `i64`. See Module 2, \"Getting Started.\"\n",
    "4. `str` - correct - The column `street` gets loaded as a `str`. See Module 2, \"Getting Started.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a5ac4-8129-4162-b8cd-5978286e8512",
   "metadata": {},
   "source": [
    "## 10.2 Question 2: Schema Override for Trip Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7830d6-7822-46c8-a574-c901f789d91f",
   "metadata": {},
   "source": [
    "Load the rides data from the CSV file, using `schema_overrides` to force `trip_distance` to be `pl.Int64`. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb345f3e-1f14-4382-8ed0-4bf267775ad4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "could not parse `0.73` as dtype `i64` at column 'trip_distance' (column number 5)\n\nThe current offset in the file is 323447842 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `dtypes` argument\n- setting `ignore_errors` to `True`,\n- adding `0.73` to the `null_values` list.\n\nOriginal error: ```remaining bytes non-empty```",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/yellow_tripdata_2024-03.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#### YOUR CODE HERE\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrip_distance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInt64\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m result_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/code/polars-oreilly-course/.venv/lib/python3.9/site-packages/polars/_utils/deprecation.py:91\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     88\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m     89\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m     90\u001b[0m     )\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/polars-oreilly-course/.venv/lib/python3.9/site-packages/polars/_utils/deprecation.py:91\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     88\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m     89\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m     90\u001b[0m     )\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/polars-oreilly-course/.venv/lib/python3.9/site-packages/polars/_utils/deprecation.py:91\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     88\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m     89\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[1;32m     90\u001b[0m     )\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/polars-oreilly-course/.venv/lib/python3.9/site-packages/polars/io/csv/functions.py:496\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m prepare_file_arg(\n\u001b[1;32m    490\u001b[0m         source,\n\u001b[1;32m    491\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    495\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[0;32m--> 496\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43m_read_csv_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnull_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8-lossy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m            \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mglob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_columns:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _update_columns(df, new_columns)\n",
      "File \u001b[0;32m~/code/polars-oreilly-course/.venv/lib/python3.9/site-packages/polars/io/csv/functions.py:642\u001b[0m, in \u001b[0;36m_read_csv_impl\u001b[0;34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    640\u001b[0m projection, columns \u001b[38;5;241m=\u001b[39m parse_columns_arg(columns)\n\u001b[0;32m--> 642\u001b[0m pydf \u001b[38;5;241m=\u001b[39m \u001b[43mPyDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_null_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43meol_char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(pydf)\n",
      "\u001b[0;31mComputeError\u001b[0m: could not parse `0.73` as dtype `i64` at column 'trip_distance' (column number 5)\n\nThe current offset in the file is 323447842 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `dtypes` argument\n- setting `ignore_errors` to `True`,\n- adding `0.73` to the `null_values` list.\n\nOriginal error: ```remaining bytes non-empty```"
     ]
    }
   ],
   "source": [
    "result_df = pl.read_csv(\n",
    "    \"../data/yellow_tripdata_2024-03.csv\",\n",
    "    schema_overrides={\n",
    "        #### YOUR CODE HERE\n",
    "        \"trip_distance\": pl.Int64\n",
    "    }\n",
    ")\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b914a-37ba-4cc9-94c3-fe48f04b3b86",
   "metadata": {},
   "source": [
    "1. An error is thrown, stating that \"`schema_overrides` only works on `str` columns\". - incorrect - An error is indeed thrown, but not this one. See Module 2, “Getting Started.”\n",
    "2. The code runs successfully, casting the would-be `float` column to `pl.Int64` upon instantiation of the dataframe. - incorrect - This code is not able to run successfully. See Module 2, “Getting Started.”\n",
    "3. An error is thrown, stating that data from the column can't be parsed to `pl.Int64`. - correct - Data that is `float` cannot be cast to `pl.Int64` upon being read. This can certainly happen later though, once the data has been read. See Module 2, “Getting Started.”\n",
    "4. The code runs successfully, ignoring the schema override and simply loading the data as `pl.Float64` - incorrect - This code is not able to run successfully. See Module 2, “Getting Started.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95738108-11b6-466d-89ed-b1d943930cdf",
   "metadata": {},
   "source": [
    "## 10.3 Question 3: Maximum Congestion Surcharge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ca719-9c3e-42c1-b229-567f86d05b36",
   "metadata": {},
   "source": [
    "What is the maximum `congestion_surcharge` in `rides_df_raw`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2afa257b-8d5f-493b-990a-47752da1631f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 1)\n",
      "┌─────────────────┐\n",
      "│ fare_amount_min │\n",
      "│ ---             │\n",
      "│ f64             │\n",
      "╞═════════════════╡\n",
      "│ 900.0           │\n",
      "└─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .select([\n",
    "        pl.col(\"fare_amount\").max().name.suffix(\"_min\")\n",
    "    ])\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6559d-0265-4988-ad4a-e53951b8394c",
   "metadata": {},
   "source": [
    "1. -2.5 - incorrect - This is the minimum `congestion_surcharge`, but the question asks for the maximum. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "2. 900.0 - incorrect - This is the maximum `fare_amount`, but the question asks for the maximum `congestion_surcharge`. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "3. 2.5 - correct - Exactly! You find this value by using the `.max()` function. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "4. 3.4 - incorrect - If you came up with this result, you may not be using the correct column. See Module 3, \"Data Manipulation I: Basics.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe7f3e-0c22-4597-9080-df74ae95ec63",
   "metadata": {},
   "source": [
    "## 10.4 Question 4: Top Trip Distance After Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92dbbd-929f-4be9-88d1-b5df9b0b4498",
   "metadata": {},
   "source": [
    "Sort `rides_df_raw` in descending order by the following columns in this order: `congestion_surcharge`, `tip_amount`, `trip_distance`. What is the `trip_distance` of the top trip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d80cc9-1534-47b4-9fd0-a76c86309004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 1)\n",
      "┌───────────────┐\n",
      "│ trip_distance │\n",
      "│ ---           │\n",
      "│ f64           │\n",
      "╞═══════════════╡\n",
      "│ 28.9          │\n",
      "└───────────────┘\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .sort(\n",
    "        by=[\n",
    "            \"congestion_surcharge\",\n",
    "            \"tip_amount\",\n",
    "            \"trip_distance\"\n",
    "        ],\n",
    "        descending=True\n",
    "    )\n",
    "    .select(\"trip_distance\")\n",
    "    .head(1)\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b917dd-384a-4627-afd0-b80796ddbf03",
   "metadata": {},
   "source": [
    "1. 176836.3 - incorrect - Make sure you're sorting by the columns in precisely the correct order: `congestion_surcharge`, `tip_amount`, and then `trip_distance`. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "2. 0.0 - incorrect - Make sure to sort in descending order, not ascending. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "3. 166.1 - incorrect - If you got this result, you probably didn't select the correct column. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "4. 28.9 - correct - Exactly! This is the top row after sorting by all the specified columns. See Module 3, \"Data Manipulation I: Basics.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077d085-0a6c-4e0d-97fc-824a1b6b2c49",
   "metadata": {},
   "source": [
    "## 10.5 Question 5: Trips Within Distance Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7d87e-a339-4017-bf5b-d5becd5344d0",
   "metadata": {},
   "source": [
    "How many trips had a `trip_distance` greater than 1 kilometer (km) and less than 2 km?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e529998-a469-4332-a753-bf0be4f64191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(838278, 1)\n"
     ]
    }
   ],
   "source": [
    "kilometers_per_mile = 1.61\n",
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .select(\n",
    "        (pl.col(\"trip_distance\") * kilometers_per_mile)#.name.suffix(\"_kilometers\")\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col(\"trip_distance\").ge(1)\n",
    "        .and_(pl.col(\"trip_distance\").lt(2))\n",
    "    )\n",
    "    .shape\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4c745-292a-4c1f-bce5-93edc7dc7201",
   "metadata": {},
   "source": [
    "1. 838278 - correct - You arrived at this answer by using `.gt()` and `.lt()`, after converting to kilometers. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "2. 1112153 - incorrect - You probably forgot to convert miles to kilometers. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "3. 1 - incorrect - You were probably thinking of the number of rows, but the question asks for the number of trips. See Module 3, \"Data Manipulation I: Basics.\"\n",
    "4. 99283 - incorrect - If you got this result, you likely did not use the correct column for `trip_distance`. See Module 3, \"Data Manipulation I: Basics.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ea5bb-3c43-4bf4-8249-94d8fe7e0cc9",
   "metadata": {},
   "source": [
    "## 10.6 Question 6: Comparison of Different Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf43e15-d2b1-4382-a66f-390521ff3b99",
   "metadata": {},
   "source": [
    "Try to add a column code that checks whether `do_zone` is greater than zero. What happens and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423343ae-bc98-4010-805a-f97e4b4c4d08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "cannot compare string with numeric type (i32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mrides_df_raw\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#### YOUR CODE HERE\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdo_zone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/code/polars-oreilly-course/.venv/lib/python3.9/site-packages/polars/dataframe/frame.py:9046\u001b[0m, in \u001b[0;36mDataFrame.with_columns\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[1;32m   8900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_columns\u001b[39m(\n\u001b[1;32m   8901\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8902\u001b[0m     \u001b[38;5;241m*\u001b[39mexprs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[1;32m   8903\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs: IntoExpr,\n\u001b[1;32m   8904\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   8905\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   8906\u001b[0m \u001b[38;5;124;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[1;32m   8907\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9044\u001b[0m \u001b[38;5;124;03m    └─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[1;32m   9045\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 9046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/polars-oreilly-course/.venv/lib/python3.9/site-packages/polars/lazyframe/frame.py:2034\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, no_optimization, streaming, engine, background, _eager, **_kwargs)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[1;32m   2033\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[0;32m-> 2034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mComputeError\u001b[0m: cannot compare string with numeric type (i32)"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .with_columns(pl.col(\"do_zone\").gt(0))\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856bc1d-852a-47fc-b305-b376706b2e41",
   "metadata": {},
   "source": [
    "1. The code crashes because a string cannot be compared with an integer - correct - To compare these two datatypes, something must be done to make them comparable, either casting the integer to a string or casting the string to an integer somehow. See Module 7, \"Data Manipulation V: Data Types.\"\n",
    "2. The code crashes because a Boolean column cannot be added to a DataFrame with `.with_columns()` - incorrect - There's no problem with adding a Boolean column to a DataFrame, and in fact you saw it done in the module! See Module 7, \"Data Manipulation V: Data Types.\"\n",
    "3. The code runs successfully, adding a column that checks whether the `do_zone` is alphabetically greater than the string \"0\" - incorrect - Polars does not do typecasting like this–you must request it explicitly. See Module 7, \"Data Manipulation V: Data Types.\"\n",
    "4. The code runs successfully; however, since strings and integers can't be compared in Polars, the resultant column is null everywhere - incorrect - Polars does not do this kind of handling, though you could probably use column expressions to do something similar. See Module 7, \"Data Manipulation V: Data Types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c602d-a0b4-4705-be48-4e102fb5ca9e",
   "metadata": {},
   "source": [
    "## 10.7 Question 7: Equivalent Renaming Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2db9a-014c-4311-8d8c-a18c7b782c5b",
   "metadata": {},
   "source": [
    "See the renaming, which uses `.select` and `.rename()`. What expression using `.select` and name transformations from the `.name` namespace would accomplish the same function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d7cc56b-16d8-4335-b3ae-0988fceb9db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌────────────────────────┬────────────────────────┐\n",
      "│ do_location_id_renamed ┆ pu_location_id_renamed │\n",
      "│ ---                    ┆ ---                    │\n",
      "│ i32                    ┆ i32                    │\n",
      "╞════════════════════════╪════════════════════════╡\n",
      "│ 239                    ┆ 142                    │\n",
      "│ 24                     ┆ 238                    │\n",
      "│ 75                     ┆ 263                    │\n",
      "│ 162                    ┆ 164                    │\n",
      "│ 7                      ┆ 263                    │\n",
      "└────────────────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "columns_to_rename = [\"do_location_id\", \"pu_location_id\"]\n",
    "df_w_cols_renamed = (\n",
    "    rides_df_raw\n",
    "    .select(columns_to_rename)\n",
    "    .rename({\n",
    "        column_to_rename: f\"{column_to_rename}_renamed\"\n",
    "        for column_to_rename in columns_to_rename\n",
    "    })\n",
    "    .head()\n",
    ")\n",
    "print(df_w_cols_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ba27f6-824a-455f-ad1a-6413ebb7c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 2)\n",
      "┌────────────────────────┬────────────────────────┐\n",
      "│ do_location_id_renamed ┆ pu_location_id_renamed │\n",
      "│ ---                    ┆ ---                    │\n",
      "│ i32                    ┆ i32                    │\n",
      "╞════════════════════════╪════════════════════════╡\n",
      "│ 239                    ┆ 142                    │\n",
      "│ 24                     ┆ 238                    │\n",
      "│ 75                     ┆ 263                    │\n",
      "│ 162                    ┆ 164                    │\n",
      "│ 7                      ┆ 263                    │\n",
      "└────────────────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "df_w_cols_renamed = (\n",
    "    rides_df_raw\n",
    "    .select(pl.col([\"do_location_id\", \"pu_location_id\"]).name.suffix(\"_renamed\"))\n",
    "    .head()\n",
    ")\n",
    "print(df_w_cols_renamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6970fe-6efd-46d0-a25c-3f655651c2ef",
   "metadata": {},
   "source": [
    "1. `.select(pl.col(pl.Int32).name.suffix(\"_renamed\"))` - incorrect - This almost works, but it accidentally includes `vendor_id` in the renaming. See Module 4, \"Data Manipulation II: Advanced Selecting.\"\n",
    "2. `.select(pl.col([\"do_location_id\", \"pu_location_id\"]).name.suffix(\"_renamed\"))` - correct - Using the `.name.suffix()` function, you can accomplish the same thing that `.rename()` does. See Module 4, \"Data Manipulation II: Advanced Selecting.\"\n",
    "3. `.select(pl.col([\"do_location_id\", \"pu_location_id\"]).alias(\"_renamed\"))` - incorrect - Using `alias()` will simply replace the column name altogether, not add a suffix. Furthermore, this code will rename both columns to the same thing and thus give an error. See Module 4, \"Data Manipulation II: Advanced Selecting.\"\n",
    "4. `.select(pl.String).name.suffix(\"_renamed\"))` - incorrect - This will select the wrong columns. See Module 4, \"Data Manipulation II: Advanced Selecting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be689bdc-a7fd-46ac-acd5-9d78e7d95c12",
   "metadata": {},
   "source": [
    "## 10.8 Question 8: Adding Suffixed Columns for Int8 Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284eee7-b925-42b1-af02-a93df959d425",
   "metadata": {},
   "source": [
    "Add a few new columns to the DataFrame that copy all `pl.Int8` columns and give them the suffix `_new`. What is the size of the resultant DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83283f2d-a551-41a7-99f8-31446d1e691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3582628, 35)\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .with_columns(pl.col(pl.Float64).name.suffix(\"_new\"))\n",
    "    .shape\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e55a8-78c7-4700-a15f-ce1d3c6bbe1a",
   "metadata": {},
   "source": [
    "1. (3582628, 28) - incorrect - You may have checked for `pl.Int32` columns instead of `pl.Int8` columns. See Module 4, \"Data Manipulation II: Advanced Selecting.\"\n",
    "2. (0, 0) - incorrect - You probably used `.select()` instead of `.with_columns()`. See Module 4, \"Data Manipulation II: Advanced Selecting.\"\n",
    "3. (3582628, 25) - correct - Exactly! There are no columns with this datatype, so the shape remains the same. See Module 4, \"Data Manipulation II: Advanced Selecting.\"\n",
    "4. (3582628, 35) - incorrect - Make sure to check for `pl.Int8` columns, not `pl.Float64` columns. See Module 4, \"Data Manipulation II: Advanced Selecting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56758951-5a23-4814-a6ec-9529112141be",
   "metadata": {},
   "source": [
    "## 10.9 Question 9: Most Common Congestion Surcharge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa12a2-77fd-4a8e-838d-e05279a8c0fc",
   "metadata": {},
   "source": [
    "Use `.group_by()` to determine the most common value for `congestion_surcharge` in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4902781-af09-4a94-8f82-0b1c9160ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (7, 2)\n",
      "┌──────────────────────┬─────────────────┐\n",
      "│ congestion_surcharge ┆ num_occurrences │\n",
      "│ ---                  ┆ ---             │\n",
      "│ f64                  ┆ u32             │\n",
      "╞══════════════════════╪═════════════════╡\n",
      "│ 2.5                  ┆ 2880218         │\n",
      "│ null                 ┆ 426190          │\n",
      "│ 0.0                  ┆ 239793          │\n",
      "│ -2.5                 ┆ 36421           │\n",
      "│ 1.0                  ┆ 3               │\n",
      "│ 0.75                 ┆ 2               │\n",
      "│ -0.75                ┆ 1               │\n",
      "└──────────────────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .group_by(\"congestion_surcharge\")\n",
    "    .agg(pl.len().alias(\"num_occurrences\"))\n",
    "    .sort(\"num_occurrences\", descending=True)\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b267d-0fd3-4716-a2f8-ca88db2a09e8",
   "metadata": {},
   "source": [
    "1. -0.75 - incorrect - This is the value of `congestion_surcharge` with the least occurrences in the dataset, but the question asks for the value that occurs the most. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\"\n",
    "2. 202.18 - incorrect - If you got this result, you probably didn't use the correct column. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\"\n",
    "3. 134.51 - incorrect - If you got this result, you probably didn't use the correct column. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\"\n",
    "4. 2.5 - correct - Exactly! You can group by `congestion_surcharge`, then aggregate for `.len()`, and finally sort. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63022fee-5609-43a4-a094-7748ada82b08",
   "metadata": {},
   "source": [
    "## 10.10 Question 10: Zero-Tip Two-Passenger Rides Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9464f-189c-4eaa-91b5-ba3f55ce28e8",
   "metadata": {},
   "source": [
    "Create a `.pivot_table()` where the rows reflect whether the `tip_amount` was 0 or not and the columns are `passenger_count`. Then, for each combination of \"tip_amount is 0\" and `passenger_count`, compute the number of instances by using `aggregate_function` \"len\". How many rides had a `tip_amount` of 0 and a `passenger_count` of 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c21f933-a009-4f3d-8aee-55816b1ed359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4_363, 12)\n",
      "┌────────────┬──────┬────────┬────────┬───┬──────┬──────┬──────┬────────┐\n",
      "│ tip_amount ┆ 0    ┆ 1      ┆ 2      ┆ … ┆ 7    ┆ 8    ┆ 9    ┆ null   │\n",
      "│ ---        ┆ ---  ┆ ---    ┆ ---    ┆   ┆ ---  ┆ ---  ┆ ---  ┆ ---    │\n",
      "│ f64        ┆ u32  ┆ u32    ┆ u32    ┆   ┆ u32  ┆ u32  ┆ u32  ┆ u32    │\n",
      "╞════════════╪══════╪════════╪════════╪═══╪══════╪══════╪══════╪════════╡\n",
      "│ 2.7        ┆ 384  ┆ 7180   ┆ 1358   ┆ … ┆ null ┆ null ┆ null ┆ 74     │\n",
      "│ 3.0        ┆ 1426 ┆ 65889  ┆ 12053  ┆ … ┆ null ┆ null ┆ null ┆ 893    │\n",
      "│ 0.0        ┆ 9740 ┆ 525349 ┆ 104748 ┆ … ┆ 4    ┆ null ┆ 1    ┆ 368549 │\n",
      "│ 1.29       ┆ 5    ┆ 640    ┆ 146    ┆ … ┆ null ┆ null ┆ null ┆ 43     │\n",
      "│ 5.13       ┆ null ┆ 504    ┆ 113    ┆ … ┆ null ┆ null ┆ null ┆ 31     │\n",
      "│ 2.04       ┆ 7    ┆ 1251   ┆ 258    ┆ … ┆ null ┆ null ┆ null ┆ 148    │\n",
      "│ 2.0        ┆ 2588 ┆ 127344 ┆ 20620  ┆ … ┆ null ┆ null ┆ null ┆ 2007   │\n",
      "│ 4.54       ┆ 12   ┆ 4747   ┆ 1102   ┆ … ┆ null ┆ null ┆ null ┆ 64     │\n",
      "│ …          ┆ …    ┆ …      ┆ …      ┆ … ┆ …    ┆ …    ┆ …    ┆ …      │\n",
      "│ 40.19      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "│ 43.84      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "│ 35.82      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "│ 20.33      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "│ 32.71      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "│ 32.97      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "│ 35.23      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "│ 41.35      ┆ null ┆ null   ┆ null   ┆ … ┆ null ┆ null ┆ null ┆ 1      │\n",
      "└────────────┴──────┴────────┴────────┴───┴──────┴──────┴──────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .with_columns([\n",
    "        pl.col(\"tip_amount\").eq(0).name.suffix(\"_eq_0\")\n",
    "    ])\n",
    "    .pivot(\n",
    "        index=\"tip_amount\",\n",
    "        on=\"passenger_count\",\n",
    "        values=\"tip_amount\",\n",
    "        aggregate_function=\"len\",\n",
    "        sort_columns=True\n",
    "    )\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6780a64-f252-47cd-b0fb-203b3f3b0f2f",
   "metadata": {},
   "source": [
    "1. 104748 - correct - Exactly right—this is the value you find by viewing the appropriate cell in the pivot table. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\"\n",
    "2. 349149 - incorrect - Close! This is the number of rides with two passengers who did give a tip, but the question asks about rides with no tip. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\"\n",
    "3. 1.7435e6 - incorrect - Your answer might look like this if you used \"sum\" instead of \"len\" as an aggregate_function. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\"\n",
    "4. 1358 - incorrect - You may have overlooked first creating a Boolean column for \"tip_amount is 0\", as instructed in the question. The pivot table is easier to read when you start with this step. See Module 5, \"Data Manipulation III: Grouping and Aggregation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c6aff-dda5-4513-92e6-24f9c73bfb72",
   "metadata": {},
   "source": [
    "## 10.11 Question 11: Left-Join Result Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370665f-4129-431e-a3bf-cd75a5c31abb",
   "metadata": {},
   "source": [
    "Join the following two DataFrames using a left-join (`restaurant_df` into `name_df`, on the name of the restaurant). What is the shape of the resultant DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f946a3-3e59-431b-875e-46d9fe8616b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 3)\n",
      "┌──────┬─────────────────────┬─────────────────┐\n",
      "│ name ┆ favorite_restaurant ┆ restaurant_city │\n",
      "│ ---  ┆ ---                 ┆ ---             │\n",
      "│ str  ┆ str                 ┆ str             │\n",
      "╞══════╪═════════════════════╪═════════════════╡\n",
      "│ dan  ┆ Pablo's Pizza       ┆ Boston          │\n",
      "│ stan ┆ Taco Time!          ┆ New York        │\n",
      "│ ran  ┆ Taco Time!          ┆ New York        │\n",
      "│ cran ┆ Pablo's Pizza       ┆ Boston          │\n",
      "└──────┴─────────────────────┴─────────────────┘\n"
     ]
    }
   ],
   "source": [
    "name_df = pl.DataFrame({\n",
    "    \"name\": [\"dan\", \"stan\", \"ran\", \"cran\"],\n",
    "    \"favorite_restaurant\": [\"Pablo's Pizza\", \"Taco Time!\", \"Taco Time!\", \"Pablo's Pizza\"]\n",
    "})\n",
    "restaurant_df = pl.DataFrame({\n",
    "    \"restaurant_name\": [\"Pablo's Pizza\", \"Taco Time!\", \"Toledo's Burritos\",],\n",
    "    \"restaurant_city\": [\"Boston\", \"New York\", \"Los Angeles\",]\n",
    "})\n",
    "result = (\n",
    "    #### YOUR CODE HERE\n",
    "    name_df\n",
    "    .join(\n",
    "        restaurant_df,\n",
    "        left_on=\"favorite_restaurant\",\n",
    "        right_on=\"restaurant_name\",\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116873f7-88a8-40ec-9e77-40b161ae1a54",
   "metadata": {},
   "source": [
    "1. (5, 3) - incorrect - To get this result, you probably joined `name_df` into `restaurant_df`. The order of the join is important! See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "2. The code doesn't run, throwing a `ColumnNotFoundError` - incorrect - This error results from trying to join using the `on` input argument. Be sure to use `left_on` and `right_on` since the `restaurant_name` column has a different name in the two DataFrames. See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "3. (4, 3) - correct - Exactly! The join columns combine into one, yielding a total of three columns. See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "4. (0, 2) - incorrect - If you got this result, you may have done an anti-join rather than a left-join. See Module 6, \"Data Manipulation IV: Combining Data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504a126-5517-4a11-abbb-f5c4ff8af60c",
   "metadata": {},
   "source": [
    "## 10.12 Question 12: Anti-Join Result Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b04f0-917c-499d-82fc-38f3d8cd85b6",
   "metadata": {},
   "source": [
    "Join the two DataFrames using an anti-join (`restaurant_df` into `name_df`, on the name of the restaurant). What is the shape of the resultant DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "661f13cd-db43-47fe-b528-828282ada825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (0, 2)\n",
      "┌──────┬─────────────────────┐\n",
      "│ name ┆ favorite_restaurant │\n",
      "│ ---  ┆ ---                 │\n",
      "│ str  ┆ str                 │\n",
      "╞══════╪═════════════════════╡\n",
      "└──────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "name_df = pl.DataFrame({\n",
    "    \"name\": [\"dan\", \"stan\", \"ran\", \"cran\"],\n",
    "    \"favorite_restaurant\": [\"Pablo's Pizza\", \"Taco Time!\", \"Taco Time!\", \"Pablo's Pizza\"]\n",
    "})\n",
    "restaurant_df = pl.DataFrame({\n",
    "    \"restaurant_name\": [\"Pablo's Pizza\", \"Taco Time!\", \"Toledo's Burritos\",],\n",
    "    \"restaurant_city\": [\"Boston\", \"New York\", \"Los Angeles\",]\n",
    "})\n",
    "result = (\n",
    "    #### YOUR CODE HERE\n",
    "    name_df\n",
    "    .join(\n",
    "        restaurant_df,\n",
    "        left_on=\"favorite_restaurant\",\n",
    "        right_on=\"restaurant_name\",\n",
    "        how=\"anti\",\n",
    "    )\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87143a5b-a59a-4813-b0f9-efddbd96f5dd",
   "metadata": {},
   "source": [
    "1. (1, 2) - incorrect - To get this result, you probably anti-joined `name_df` into `restaurant_df`. The order of the anti-join is important! See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "2. The code doesn't run, throwing a `ColumnNotFoundError` - incorrect - This error results from trying to join using the `on` input argument. Be sure to use `left_on` and `right_on` since the `restaurant_name` column has a different name in the two DataFrames. See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "3. (4, 3) - incorrect - If you got this result, you may have used a left-join. See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "4. (0, 2) - correct - Exactly! There is nothing in `names_df` that is missing a row to join with in `restaurant_df`. See Module 6, \"Data Manipulation IV: Combining Data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8babe2e-be44-43a2-b3b6-6e53ac374583",
   "metadata": {},
   "source": [
    "## 10.13 Question 13: Null Count After Diagonal Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b41682-823e-4c9b-9834-c187b1cff248",
   "metadata": {},
   "source": [
    "Vertically concatenate the DataFrames (with setting `\"how=diagonal\"`). How many null values are there in the resultant DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e17361f-97a7-4522-9f07-c9cdcede53be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 1)\n",
      "┌─────┐\n",
      "│ A   │\n",
      "│ --- │\n",
      "│ u32 │\n",
      "╞═════╡\n",
      "│ 20  │\n",
      "└─────┘\n"
     ]
    }
   ],
   "source": [
    "df1 = pl.DataFrame({\n",
    "    \"A\": [1, 2, None, 4],\n",
    "    \"B\": [5, None, 7, 8]\n",
    "})\n",
    "df2 = pl.DataFrame({\n",
    "    \"A\": [9, 10, None],\n",
    "    \"B\": [None, 13, 14],\n",
    "    \"C\": [15, 16, 17]\n",
    "})\n",
    "df3 = pl.DataFrame({\n",
    "    \"A\": [18, None, 20],\n",
    "    \"B\": [21, 22, 23],\n",
    "    \"D\": [24, 25, None]\n",
    "})\n",
    "#### YOUR CODE HERE\n",
    "result = (\n",
    "    pl.concat([df1, df2, df3], how=\"diagonal\")\n",
    "    .null_count()\n",
    "    .select(pl.sum_horizontal(pl.all()))\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8202f3-7a9c-4e5d-b7a1-d65bc08383d9",
   "metadata": {},
   "source": [
    "1. 20 - correct - Exactly! There are a few nulls in the DataFrame to begin with, and there are even more after a diagonal concatenation. See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "2. 6 - incorrect - This is the number of nulls across the DataFrames to begin with. The concatenation adds more. See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "3. The code doesn't run due to a `ShapeError` - incorrect - Some DataFrames have nonoverlapping columns, so make sure to set how as diagonal. See Module 6, \"Data Manipulation IV: Combining Data.\"\n",
    "4. 0 - incorrect - There are null values in the DataFrame to begin with, and the concatenation adds more. Did you replace them with something else somehow? See Module 6, \"Data Manipulation IV: Combining Data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a1a44-9369-43b6-8b01-6cf27d802fe5",
   "metadata": {},
   "source": [
    "## 10.14 Question 14: Fraction of Early March Rides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd797d-eb94-454d-81be-04935b405090",
   "metadata": {},
   "source": [
    "What fraction of rides in the DataFrame started before 2024-03-15 (i.e., had a `tpep_pickup_datetime` before 2024-03-15)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f7dab26-6b63-4095-a1c9-1f5ee9443b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 1)\n",
      "┌──────────────────────┐\n",
      "│ tpep_pickup_datetime │\n",
      "│ ---                  │\n",
      "│ f64                  │\n",
      "╞══════════════════════╡\n",
      "│ 0.455173             │\n",
      "└──────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .select(pl.col(\"tpep_pickup_datetime\").lt(pl.date(2024, 3, 15)).mean())\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74939e2-8d19-44c2-b13a-240940057f47",
   "metadata": {},
   "source": [
    "1. 0.455174 - incorrect - You may have used `.le()` rather than `.lt()` to find the rides that started before the given date. See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "2. 0.455173 - correct - You can do a commonsense check to confirm: March 15 is almost halfway through the month, so it makes sense that the number here is almost 0.5! See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "3. 1.0 - incorrect - The value of 1.0 represents all the rides! Ensure you are capturing just the fraction of rides specified. See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "4. 1630716 - incorrect - The question asks for the fraction of rides that started before March 15, 2024, not the number of rides. See Module 9, \"Integrating with the Data Science Workflow.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f111af0-ee73-4700-90e9-e7d1dcca4ad0",
   "metadata": {},
   "source": [
    "## 10.15 Question 15: List Column Aggregation Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59a710-478d-4201-b3bf-f8db66a48804",
   "metadata": {},
   "source": [
    "Group the following DataFrame by `class`, and `.sum()` the `salient_appendages` column (which is a list type) to create a list of all `salient_appendages` for that animal class. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2a15bef-e323-4d6a-8cf1-b6b14a2ddc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n",
      "┌────────────────┬────────────────────┐\n",
      "│ class          ┆ salient_appendages │\n",
      "│ ---            ┆ ---                │\n",
      "│ str            ┆ list[str]          │\n",
      "╞════════════════╪════════════════════╡\n",
      "│ mammal         ┆ null               │\n",
      "│ chondrichthyes ┆ null               │\n",
      "└────────────────┴────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "animal_appendages_df = pl.DataFrame({\n",
    "    \"animal\": [\"human\", \"elephant\", \"shark\", \"stingray\"],\n",
    "    \"class\": [\"mammal\", \"mammal\", \"chondrichthyes\", \"chondrichthyes\"],\n",
    "    \"salient_appendages\": [\n",
    "        [\"arm\", \"leg\", \"head\"],\n",
    "        [\"arm\", \"leg\", \"head\", \"trunk\"],\n",
    "        [\"fin\", \"tail\", \"jaw\"],\n",
    "        [\"fin\", \"tail\", \"stinger\"],\n",
    "    ]\n",
    "})\n",
    "result = (\n",
    "    #### YOUR CODE HERE\n",
    "    animal_appendages_df\n",
    "    .group_by(\"class\")\n",
    "    .agg(pl.col(\"salient_appendages\").sum())\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87ed11-ef36-4c12-8766-6e20c5910260",
   "metadata": {},
   "source": [
    "1. The code runs smoothly, but the resultant aggregated column is filled with nulls - correct - Clearly a different approach to concatenating the lists is needed. As you advance in your mastery of Polars, you will learn more about this. See Module 7, \"Data Manipulation V: Data Types.\"\n",
    "2. The code runs smoothly, and the resultant column is a combined list of all `salient_appendages` for that animal class - incorrect - This is the desired result, but it requires an advanced technique to achieve. See Module 7, \"Data Manipulation V: Data Types.\"\n",
    "3. The code crashes with a datatype error, saying that \"list columns cannot be added together in a group_by\" - incorrect - Indeed, the code doesn't successfully produce the desired outcome. However, it doesn't throw an error. See Module 7, \"Data Manipulation V: Data Types.\"\n",
    "4. The code runs smoothly, and the resultant column is a list of lists of all values for `salient_appendages` for that animal class - incorrect - This is what would happen if you didn't use the `.sum()` function and just left the aggregation as a simple `.agg(pl.col(\"salient_appendages\"))`. See Module 7, \"Data Manipulation V: Data Types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bff3c4-f75e-4ce5-829e-70e7d0c9b9ea",
   "metadata": {},
   "source": [
    "## 10.16 Question 16: Weekend vs Weekday Tip Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a88f4a-34ef-48bd-a7c7-fa39aeb389af",
   "metadata": {},
   "source": [
    "True or False: weekend taxi trips (trips that have a `tpep_pickup_datetime` on Saturday or Sunday) have on average higher tip amounts than non-weekend taxi trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a583e12c-1f95-4d80-8ec3-fb0b005ddf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n",
      "┌───────────────────┬────────────┐\n",
      "│ is_weekend_pickup ┆ tip_amount │\n",
      "│ ---               ┆ ---        │\n",
      "│ bool              ┆ f64        │\n",
      "╞═══════════════════╪════════════╡\n",
      "│ true              ┆ 2.966726   │\n",
      "│ false             ┆ 3.294616   │\n",
      "└───────────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    rides_df_raw\n",
    "    #### YOUR CODE HERE\n",
    "    .with_columns(\n",
    "        pl.col(\"tpep_pickup_datetime\").dt.weekday().eq(6)\n",
    "        .or_(pl.col(\"tpep_pickup_datetime\").dt.weekday().eq(7))\n",
    "        .alias(\"is_weekend_pickup\")\n",
    "    )\n",
    "    .group_by(\"is_weekend_pickup\")\n",
    "    .agg(pl.col(\"tip_amount\").mean())\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5176f6-3709-4330-99f1-accd1b649dde",
   "metadata": {},
   "source": [
    "1. False - correct - Exactly! Weekend pickups have an average `tip_amount` of 2.97, while non-weekend pickups have an average `tip_amount` of 3.29. See Module 7, \"Data Manipulation V: Data Types.\"\n",
    "2. True - incorrect - If you found the statement to be true, you may not have created the column for grouping correctly. See Module 7, \"Data Manipulation V: Data Types.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55509675-c102-44bf-8f54-59ee40eb4181",
   "metadata": {},
   "source": [
    "## 10.17 Question 17: NumPy to Polars DataFrame Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0414e6-e96c-4678-a231-d78cf450496e",
   "metadata": {},
   "source": [
    "Convert the following NumPy array to a Polars DataFrame. What are the datatypes of the resultant DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "699c0aa7-5053-457d-b5de-2fde779b8c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th><th>column_1</th></tr><tr><td>f64</td><td>f64</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 2)\n",
       "┌──────────┬──────────┐\n",
       "│ column_0 ┆ column_1 │\n",
       "│ ---      ┆ ---      │\n",
       "│ f64      ┆ f64      │\n",
       "╞══════════╪══════════╡\n",
       "└──────────┴──────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_array = np.random.rand(4, 2)\n",
    "#### YOUR CODE HERE\n",
    "pl.from_numpy(np_array).head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aaef93-048f-4940-a64d-77fc7daacb6c",
   "metadata": {},
   "source": [
    "1. `f64`, `f64` - correct - Exactly! The data were Float64's in the NumPy array, and they stayed that way when they got converted to a Polars DataFrame. See Module 8, \"Data Manipulation VI: Interoperation and IO.\"\n",
    "2. `f32`, `f32` - incorrect - Despite being random floats between 0 and 1, the floats are still 64-bit precision. See Module 8, \"Data Manipulation VI: Interoperation and IO.\"\n",
    "3. `f64`, `f64`, `f64`, `f64` - incorrect - If you got this result, you may have transposed the data somehow. See Module 8, \"Data Manipulation VI: Interoperation and IO.\"\n",
    "4. The code doesn't run due to a datatype conversion error - incorrect - There is no problem with creating a Polars DataFrame from a NumPy array. See Module 8, \"Data Manipulation VI: Interoperation and IO.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1832ad77-3de1-4b47-9885-e60e59c06e87",
   "metadata": {},
   "source": [
    "## 10.18 Question 18: Polars to Pandas to Polars Series Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6514195-c4d7-4e3e-a970-271e00fb5292",
   "metadata": {},
   "source": [
    "Create a Polars series, convert it to Pandas, and then convert it back to Polars. True or False: the result upon returning to Polars is now a single-column DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "859355f6-7eed-4c88-be37-97fe0cc935b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>polars_series</th></tr><tr><td>i64</td></tr></thead><tbody><tr><td>0</td></tr><tr><td>1</td></tr><tr><td>2</td></tr><tr><td>3</td></tr><tr><td>4</td></tr><tr><td>5</td></tr><tr><td>6</td></tr><tr><td>7</td></tr><tr><td>8</td></tr><tr><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10,)\n",
       "Series: 'polars_series' [i64]\n",
       "[\n",
       "\t0\n",
       "\t1\n",
       "\t2\n",
       "\t3\n",
       "\t4\n",
       "\t5\n",
       "\t6\n",
       "\t7\n",
       "\t8\n",
       "\t9\n",
       "]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polars_series = pl.Series(name=\"polars_series\", values=list(range(10)))\n",
    "#### YOUR CODE HERE\n",
    "pl.from_pandas(polars_series.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec8dc3e-8c8c-4f8b-97e6-3531f45840fa",
   "metadata": {},
   "source": [
    "1. True - incorrect - If you are seeing a single column, perhaps you accidentally converted it to a DataFrame yourself. See Module 8, \"Data Manipulation VI: Interoperation and IO.\"\n",
    "2. False - correct - Exactly! Polars handles this type of interoperation without a problem, cleanly able to convert a Polars series to a Pandas series and back. See Module 8, \"Data Manipulation VI: Interoperation and IO.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f994737-61c6-4f40-ae6f-430e21bf972f",
   "metadata": {},
   "source": [
    "## 10.19 Question 19: Feature Most Correlated with Tip Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c23ef4-a3db-4792-90e3-becfe1bdf123",
   "metadata": {},
   "source": [
    "Using `rides_df_raw`, which feature is most highly correlated (either positively or negatively) with `tip_amount` (excluding `tip_amount` itself)? Also, please filter out `null` values as done in the module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e78c4812-726c-478b-b1ef-094139136c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>tip_amount</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;passenger_count&quot;</td><td>NaN</td></tr><tr><td>&quot;ratecode_id&quot;</td><td>NaN</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌─────────────────┬────────────┐\n",
       "│                 ┆ tip_amount │\n",
       "│ ---             ┆ ---        │\n",
       "│ str             ┆ f64        │\n",
       "╞═════════════════╪════════════╡\n",
       "│ passenger_count ┆ NaN        │\n",
       "│ ratecode_id     ┆ NaN        │\n",
       "└─────────────────┴────────────┘"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE\n",
    "corr = (\n",
    "    rides_df_raw\n",
    "    .select(pl.col([pl.Int32, pl.Int64, pl.Float64]))\n",
    "    # .filter(\n",
    "    #     pl.all_horizontal(pl.all().is_not_null())\n",
    "    # )\n",
    "    .corr()\n",
    ")\n",
    "(\n",
    "    corr\n",
    "    .select(\n",
    "        pl.Series(corr.columns),\n",
    "        \"tip_amount\"\n",
    "    )\n",
    "    .sort(\n",
    "        pl.col(\"tip_amount\").abs(),\n",
    "        descending=True\n",
    "    )\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecffec3-56c3-4225-b15d-fb25346c2342",
   "metadata": {},
   "source": [
    "1. `tip_amount` - incorrect - If this were included in the scope of the question, it would be correct since a value always correlates perfectly with itself. However, the question explicitly excludes self-correlation. See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "2. `total_amount` - correct - Indeed, `total_amount` is most highly correlated with `tip_amount`. This makes sense since `total_amount` is a sum that includes `tip_amount`! See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "3. `congestion_surcharge` - incorrect - This is the feature least correlated with `tip_amount`. You may have made an error in your filtering, sorting, and/or selecting. See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "4. `passenger_count` - incorrect - If you got this result, you may have overlooked the instruction to filter out nulls before checking the correlations. See Module 9, \"Integrating with the Data Science Workflow.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf390c2-87f4-44d1-a87a-16bc0cff0c25",
   "metadata": {},
   "source": [
    "## 10.20 Question 20: Hourly Average Total Amount Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1daf6a6-5d33-48a0-9cb8-ad0ce3b38950",
   "metadata": {},
   "source": [
    "With `rides_df_raw`, make a plot of 'hour of day of taxi ride' vs 'average total_amount'. Which of the following statements is True (hint: there are multiple options)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3897989a-bb14-4142-ab3c-5f057f0bb13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-befaa1c048d54236be3283003226a1a0.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-befaa1c048d54236be3283003226a1a0.vega-embed details,\n",
       "  #altair-viz-befaa1c048d54236be3283003226a1a0.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-befaa1c048d54236be3283003226a1a0\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-befaa1c048d54236be3283003226a1a0\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-befaa1c048d54236be3283003226a1a0\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-e61727e2e635b9458f86b0b01c62c013\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"x\": {\"field\": \"tpep_pickup_hour\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"average_total_amount\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_1\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-e61727e2e635b9458f86b0b01c62c013\": [{\"tpep_pickup_hour\": 3, \"average_total_amount\": 24.318270379560154}, {\"tpep_pickup_hour\": 6, \"average_total_amount\": 30.60414729924566}, {\"tpep_pickup_hour\": 9, \"average_total_amount\": 25.78426864231599}, {\"tpep_pickup_hour\": 18, \"average_total_amount\": 26.882790779400526}, {\"tpep_pickup_hour\": 0, \"average_total_amount\": 27.268029498415082}, {\"tpep_pickup_hour\": 15, \"average_total_amount\": 28.661067598010636}, {\"tpep_pickup_hour\": 21, \"average_total_amount\": 26.015457538246718}, {\"tpep_pickup_hour\": 12, \"average_total_amount\": 26.388060886189557}, {\"tpep_pickup_hour\": 19, \"average_total_amount\": 26.681189991246004}, {\"tpep_pickup_hour\": 4, \"average_total_amount\": 29.032333526584868}, {\"tpep_pickup_hour\": 13, \"average_total_amount\": 27.140467165763013}, {\"tpep_pickup_hour\": 10, \"average_total_amount\": 26.102965456521353}, {\"tpep_pickup_hour\": 16, \"average_total_amount\": 29.975406820276586}, {\"tpep_pickup_hour\": 7, \"average_total_amount\": 26.619156089515425}, {\"tpep_pickup_hour\": 1, \"average_total_amount\": 24.621474829208072}, {\"tpep_pickup_hour\": 22, \"average_total_amount\": 26.947616657221854}, {\"tpep_pickup_hour\": 23, \"average_total_amount\": 28.566955722605844}, {\"tpep_pickup_hour\": 17, \"average_total_amount\": 28.299014035394134}, {\"tpep_pickup_hour\": 20, \"average_total_amount\": 25.864373175315002}, {\"tpep_pickup_hour\": 8, \"average_total_amount\": 25.297219812028647}, {\"tpep_pickup_hour\": 2, \"average_total_amount\": 23.32475528811872}, {\"tpep_pickup_hour\": 11, \"average_total_amount\": 25.985248219750755}, {\"tpep_pickup_hour\": 5, \"average_total_amount\": 34.02273595505469}, {\"tpep_pickup_hour\": 14, \"average_total_amount\": 28.457525878456817}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE\n",
    "(\n",
    "    rides_df_raw\n",
    "    .group_by(pl.col(\"tpep_pickup_datetime\").dt.hour().alias(\"tpep_pickup_hour\"))\n",
    "    .agg(pl.col(\"total_amount\").mean().name.prefix(\"average_\"))\n",
    "    .plot\n",
    "    .line(\n",
    "        x=\"tpep_pickup_hour\",\n",
    "        y=\"average_total_amount\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a3e74-5579-40c1-806a-58749a24853f",
   "metadata": {},
   "source": [
    "1. There appears to be a peak in average total amount around 2:00 - incorrect - There is no peak around 2:00 a.m.. In fact, this is the hour with the smallest `average_total_amount`. See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "2. There appears to be a peak in average total amount around 5:00 - correct - There is indeed a peak at 5:00 a.m.! Check the plot against your x-axis. See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "3. There appears to be a peak in average total amount around 16:00 - correct - There is indeed a peak at 4:00 p.m.! Check the plot against your x-axis. See Module 9, \"Integrating with the Data Science Workflow.\"\n",
    "4. There appears to be a peak in average total amount around 23:00 - correct - There is indeed a peak at 11:00 p.m.! Check the plot against your x-axis. See Module 9, \"Integrating with the Data Science Workflow.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
